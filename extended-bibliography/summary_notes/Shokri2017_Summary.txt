Title: Membership Inference Attacks against Machine Learning Models 
Authors: Reza Shokri, Marco Stronati, Congzheng Song, Vitaly Shmatikov  
Year: 2017  
Source: https://arxiv.org/abs/1610.05820

Snapshot:
This seminal work reveals that machine learning models can leak information about specific data points used during training.
It introduces membership inference attacks, showing that attackers can determine whether a record was part of a model’s training dataset—particularly
when models are overfitted or expose detailed confidence scores.


Key Findings:

•	Demonstrated that even black-box models offered as “Machine Learning as a Service” (MLaaS) are vulnerable.
• Attackers can infer membership without prior knowledge of data distribution using prediction confidence patterns.
• Highlights overfitting and excessive confidence reporting as key enablers of leakage.


Limitations:

• Assumes the attacker has query access to the target model and can observe confidence vectors.
• Focused mainly on MLaaS systems; limited model and dataset diversity.
 

Future Work:

• Investigate factors influencing membership leakage and evaluate defense effectiveness.
• Develop mitigation strategies such as differential privacy, regularization, or model auditing.


Practical Applications:

Motivates privacy-preserving training through Differentially Private SGD and regularization.
Influenced subsequent research on DP, privacy auditing, and ML model risk assessments.

